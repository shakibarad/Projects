{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d04b6e7",
   "metadata": {},
   "source": [
    "### Text Classification And Sentiment Analysis On Hotel Reviews\n",
    "This project aimed to build text classification methods on the domain of hotel reviews.The goal of this project is to implement classifiers that predict the rating of the reviews. Text classification methods were applied to predict ratings using  vectorization algorithms: term frequency-inverse document frequency (tf-idf). The focus of this project is training supervised learning text classification models to see whether or not its possible to predict reviews ratings. The motivation of this project is to predict review ratings using Logistic Regression classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cddfbd7",
   "metadata": {},
   "source": [
    "### Strategy and Process\n",
    "- pre-processing, Stopword, punctuation removal etc.\n",
    "- LabelEncoder on the review column.\n",
    "- Making a model to check the sentiment analysis with text (text column and check if the review column). \n",
    "- polarity check.\n",
    "- Making a classification machine learning model and check if the model gives better performance.\n",
    "- Making a CSV file from the text (review) column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177adec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Machine learning model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk import corpus\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import brown, stopwords\n",
    "from nltk import pos_tag\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "from textblob import TextBlob\n",
    "\n",
    "import os\n",
    "#os.getcwd()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.downloader.download('maxent_ne_chunker')\n",
    "nltk.downloader.download('words')\n",
    "nltk.downloader.download('treebank')\n",
    "nltk.downloader.download('maxent_treebank_pos_tagger')\n",
    "nltk.downloader.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data \n",
    "data = pd.read_csv(r\"hotel_reviews.csv\", encoding=\"utf-8\")\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797adf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns=[\"language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2027452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feedback = [1 if each == \"positive\" else 0 for each in data.feedback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c716aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c64aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding any NaN values\n",
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01164d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title(\"Percentage Distributions by Review Type\")\n",
    "g = plt.pie(round(data.feedback.value_counts(normalize=True)*100,2),explode=(0.025,0.025), labels=round(data.feedback.value_counts(normalize=True)*100,2).index, colors=[\"c\",\"m\"],autopct=\"%1.1f%%\", startangle=180)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0215a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontract_text(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"won\\’t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\’t\", \"can not\", text)\n",
    "    text = re.sub(r\"\\'t've\", \" not have\", text)\n",
    "    text = re.sub(r\"\\'d've\", \" would have\", text)\n",
    "    text = re.sub(r\"\\'clock\", \"f the clock\", text)\n",
    "    text = re.sub(r\"\\'cause\", \" because\", text)\n",
    "# general\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"n\\’t\", \" not\", text)\n",
    "    text = re.sub(r\"\\’re\", \" are\", text)\n",
    "    text = re.sub(r\"\\’d\", \" would\", text)\n",
    "    text = re.sub(r\"\\’ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\’t\", \" not\", text)\n",
    "    text = re.sub(r\"\\’ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\’m\", \" am\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f732a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"title\"] = data[\"title\"].apply(lambda x: decontract_text(x))\n",
    "data[\"text\"] = data[\"text\"].apply(lambda x: decontract_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69bd7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def clean_text_round1(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub('[(%s)]' % re.escape(string.punctuation), '', t)\n",
    "    t = re.sub('\\w*\\d\\w*', '', t)\n",
    "    t = re.sub('\\n', '', t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29063f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"title\"] = data[\"title\"].apply(lambda x: clean_text_round1(x))\n",
    "data[\"text\"] = data[\"text\"].apply(lambda x: clean_text_round1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbda8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def clean_text_round2(text):\n",
    "    for i in text:\n",
    "        words = i.split()\n",
    "        dialog_words = [word for word in words if word not in stop_words]\n",
    "        words1 = [ps.stem(word) for word in dialog_words]\n",
    "        dialog1 = ' '.join(words1)\n",
    "        corpus.append(dialog1)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c02839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_title=clean_text_round2(data[\"title\"].loc[data[\"feedback\"]==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055cbbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tag=[]\n",
    "for i in corpus_title:\n",
    "    words = i.split()\n",
    "    t=nltk.pos_tag(words)\n",
    "    negative_tag.append(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc67d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_noun=[]\n",
    "for i in negative_tag:\n",
    "    for j in i:\n",
    "        if j[1]=='NN':\n",
    "            negative_noun.append(j[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83eb4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {}\n",
    "for item in negative_noun:\n",
    "    if item in d1:\n",
    "        d1[item] += 1\n",
    "    else:\n",
    "        d1[item] = 1\n",
    "#dict(sorted(d1.items(), key=lambda item: item[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34a8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_title = \" \".join(i for i in corpus_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2269fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud1 = WordCloud(background_color='white', width=3000, height=2500).generate(n_title)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(wordcloud1)\n",
    "plt.axis('off')\n",
    "plt.title(\"Words which indicate negative title \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a7003",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_adjective=[]\n",
    "for i in negative_tag:\n",
    "    for j in i:\n",
    "        if j[1]=='JJ':\n",
    "            negative_adjective.append(j[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc474a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2= {}\n",
    "for item in negative_adjective:\n",
    "    if item in d2:\n",
    "        d2[item] += 1\n",
    "    else:\n",
    "        d2[item] = 1\n",
    "#dict(sorted(d2.items(), key=lambda item: item[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc404da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text=clean_text_round2(data[\"text\"].loc[data[\"feedback\"]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cec3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagword_p=[]\n",
    "for i in corpus_text:\n",
    "    words = i.split()\n",
    "    t=nltk.pos_tag(words)\n",
    "    tagword_p.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54748ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding positive adjective from positive feedbacks\n",
    "positive_adj=[]\n",
    "for i in tagword_p:\n",
    "    for j in i:\n",
    "        if j[1]=='JJ':\n",
    "            positive_adj.append(j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d62547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Words\n",
    "d1= {}\n",
    "for item in positive_adj:\n",
    "    if item in d1:\n",
    "        d1[item] += 1\n",
    "    else:\n",
    "        d1[item] = 1\n",
    "#dict(sorted(d1.items(), key=lambda item: item[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_reviews = \" \".join(i for i in corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05347846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud1 = WordCloud(background_color='white', width=3000, height=2500).generate(p_reviews)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(wordcloud1)\n",
    "plt.axis('off')\n",
    "plt.title(\"Words which indicate positive_feedback \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef278e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c99da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "for i in corpus_text:\n",
    "\n",
    "    blob = TextBlob(i)\n",
    "    print(blob.sentences)\n",
    "\n",
    "    print('\\n', blob.words)\n",
    "    print('\\n', blob.tags)\n",
    "\n",
    "    print('\\n', blob.noun_phrases) \n",
    "\n",
    "for sentence in blob.sentences:\n",
    "    print('\\nnoun phrases in sentence : ' ,sentence.noun_phrases)\n",
    "    print(sentence.sentiment)\n",
    "    print('\\ntext sentiment: ', blob.sentiment)\n",
    "    print('\\nFind the start point of Rings: ', blob.find('Rings'))\n",
    "    print('\\nsingular of word Rings: ' , blob.words[4].singularize())\n",
    "    print('\\ncount appearance of word Lord: ' , blob.words.count('Lord'))\n",
    "    print('\\nplural of word Lord: ', blob.words[1].pluralize())\n",
    "    print('\\nroot word written: ', blob.words[-7].lemmatize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94685c0",
   "metadata": {},
   "source": [
    "### polarity and subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ce1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scores_analyis=[]\n",
    "polarity=[]\n",
    "subjectivity=[]\n",
    "for i in data['text']:\n",
    "    TextBlob_Subjectivity = TextBlob(i).sentiment.subjectivity\n",
    "    subjectivity.append(TextBlob_Subjectivity)\n",
    "    TextBlob_Polarity = TextBlob(i).sentiment.polarity\n",
    "    polarity.append(TextBlob_Polarity)\n",
    "    def getAnalysis(score):\n",
    "        if score < 0:\n",
    "            return 0\n",
    "       \n",
    "        else:\n",
    "            return 1\n",
    "    TextBlob_Analysis = getAnalysis(TextBlob_Polarity)\n",
    "    Scores_analyis.append(TextBlob_Analysis)\n",
    "    \n",
    "data['subjectivity']=subjectivity\n",
    "data['polarity'] = polarity\n",
    "data['Scores_analyis']=Scores_analyis\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f580da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "data['Scores_analyis'] = lb.fit_transform(data['Scores_analyis'])\n",
    "y_pred = data['Scores_analyis'].values\n",
    "y = data['feedback'].values\n",
    "\n",
    "print('Confusion matrix : \\n', confusion_matrix(y, y_pred))\n",
    "print('Classification report: \\n', classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b721fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = \" \".join(i for i in data.text)\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "mask = np.array(Image.open('Dolfine.png'))\n",
    "wordcloud = WordCloud(width= 1500, height = 1000, random_state=1, background_color='black', \n",
    "                      colormap='rainbow', collocations=False, stopwords = STOPWORDS, mask=mask).generate(texts)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x, y = data[\"text\"], data['feedback']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1,random_state=42)\n",
    "print(f'x_train: {len(x_train)}')\n",
    "print(f'x_test: {len(x_test)}')\n",
    "print(f'y_train: {len(y_train)}')\n",
    "print(f'y_test: {len(y_test)}')\n",
    "x_train : 35038\n",
    "x_test  : 3894\n",
    "y_train : 35038\n",
    "y_test  : 3894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "tvec = TfidfVectorizer()\n",
    "clf2 = LogisticRegression(solver = \"lbfgs\")\n",
    "from sklearn.pipeline import Pipeline\n",
    "model = Pipeline([(\"vectorizer\",tvec),(\"classifier\",clf2)])\n",
    "model.fit(x_train, y_train)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predictions = model.predict(x_test)\n",
    "confusion_matrix(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e22886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "y_pred = model.predict(x_test)\n",
    "print(f'Accurcy: {accuracy_score(y_pred, y_test)}')\n",
    "print(f'Precision: {precision_score(y_pred, y_test, average=\"weighted\")}')\n",
    "print(f'Recall: {recall_score(y_pred, y_test, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278297b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting values\n",
    "from random import randint\n",
    "row = randint(0,data.text.shape[0]-1)\n",
    "sample_text = data.text[row]\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c5cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([sample_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "row = randint(0,data.text.shape[0]-1)\n",
    "sample_text = data.text[row]\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1970f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([sample_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e593809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03565f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locationtagger\n",
    "Country_cities=[]\n",
    "for i in data[\"text\"]:\n",
    "    place_entity = locationtagger.find_locations(text =i)\n",
    "    Country_cities.append(place_entity.country_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Duplicate_words=[]\n",
    "Length_words=[] \n",
    "Stop_words=[]\n",
    "Noun_words=[]\n",
    "Adjective_words=[]\n",
    "for i in data[\"text\"]:\n",
    "    words = word_tokenize(i)\n",
    "    Stop_words.append([word for word in words if word  in stop_words])\n",
    "    Length_words.append(len(words))\n",
    "    Duplicate_words.append(sorted(Counter(words) - Counter(set(words))))\n",
    "    tag_words=nltk.pos_tag(words)\n",
    "    Noun_words.append([tag[0] for tag in tag_words if tag[1]==\"NN\"])\n",
    "    Adjective_words.append([tag[0] for tag in tag_words if tag[1]==\"JJ\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee5a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(list(zip(Stop_words, Duplicate_words,Length_words, Noun_words, Adjective_words, Country_cities)),\n",
    "               columns =['Stop_words', 'Duplicate_words', 'Length_words', 'Noun_words', 'Adjective_words', 'Country_cities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735059a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"Hotels_NLP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9c4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020e358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c3320e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a978540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b1a3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
